Thanks! I’ll put together a detailed plan to evolve TunaCode’s architecture to match Claude Code as closely as possible. This will include implementation phases focused on read/write handling, context retention, streaming UI, permission gating, tool extensibility, and parallel execution.

I'll let you know as soon as the phased plan is ready.

# Implementation Roadmap for Aligning TunaCode with Claude Code Architecture

This roadmap outlines a multi-phase plan to evolve the TunaCode system to mirror the architecture and capabilities of Anthropic’s Claude Code CLI. Each phase details the engineering tasks, architectural changes, and testing requirements needed to replicate Claude Code’s functionality and user experience. The phases are sequenced with rationale to build safely and incrementally towards a Claude Code-like agentic system.

## Phase 1: Tool Classification & Concurrency Overhaul

In **Phase 1**, we will refactor TunaCode’s tool system to distinguish **read-only** operations from **write** (state-altering) operations. This lays the foundation for safe parallel execution and prevents conflicting actions. By marking tools with an `isReadOnly` attribute and adjusting the execution engine, TunaCode can mirror Claude Code’s “smart concurrency” strategy. The goal is to improve performance for safe operations while maintaining strict order for destructive actions.

**Key Tasks:**

- **Add Read/Write Metadata:** Define an `isReadOnly()` property on each tool to declare if it only reads data or can modify state (files, system, etc.). For example, a `ViewFileTool` returns `true` (read-only) while an `EditFileTool` returns `false`.
- **Concurrent vs. Sequential Execution:** Implement a scheduler that checks all tools requested in a query. If **all tools are read-only**, execute them in parallel to speed up responses. Limit concurrency (e.g. max 5–10 simultaneous ops) to balance throughput and resource usage. If **any tool is a write operation**, fall back to sequential execution in the given order to avoid race conditions.
- **Context-Aware File Operations:** Ensure file system tools operate relative to the current project directory and respect context (preventing out-of-scope file access). This may involve passing the working directory into tool calls or scoping their path access.
- **Result Ordering:** Even when using parallel execution, collect and **sort results in the original request order** before returning them to the AI. This preserves predictability in outputs despite concurrent execution.

**Testing & Validation:**

- Develop unit tests for the tool scheduler: verify that a set of only read-only tools runs concurrently and returns correctly ordered results, and that any mix including a write tool runs sequentially (no interleaving).
- Simulate a query that triggers multiple file reads (e.g. search across files) and measure execution time before vs. after concurrency – it should dramatically improve with parallelism. Also ensure the results remain correct and complete.
- Test a scenario with consecutive write operations (e.g. edit multiple files): confirm they execute one at a time in sequence and the final state is consistent with the intended edits. No race conditions or missed updates should occur.
- Perform code reviews to ensure all existing tools are correctly classified and that any new tools must declare read/write status (enforced via interface).

_Rationale:_ Establishing clear tool classifications and a concurrency model upfront provides immediate performance benefits and a safety net for later phases. This phase aligns TunaCode’s core execution loop with Claude Code’s approach to run independent read tasks in parallel and serialize state-changing tasks. It creates the groundwork for adding advanced features (like permissions and multi-step actions) on a robust, race-free tool execution framework.

## Phase 2: Integrated Permission System

Phase 2 introduces a **permission gating layer** to prevent destructive actions without user consent, closely following Claude Code’s security model. Every tool invocation that can modify the user’s system will require explicit approval, while benign read-only actions may execute automatically. This ensures TunaCode won’t perform potentially dangerous file edits or shell commands without the user’s knowledge, building trust and safety into the system.

**Key Tasks:**

- **Permission Flags in Tools:** Augment tool definitions with a `needsPermissions(input) -> boolean` function. Tools should declare if a given invocation requires user approval. For example, a file edit tool might return true if the target file path hasn’t been approved for writing yet.
- **Permission Prompt Mechanism:** Implement a permission request workflow. When the AI attempts to use a tool with `needsPermissions == true`, pause the AI’s response and prompt the user to **Allow or Deny** the action before proceeding. Visually, this can be a yes/no confirmation dialog in the CLI (to be integrated with the Phase 4 UI). Include context in the prompt (e.g. “TunaCode wants to edit file `/proj/src/config.js` – allow?”).
- **Persistent Approval Storage:** Enable saving approvals to avoid repeatedly prompting for the same action. For instance, if the user grants write access to a directory or permission to run a specific command, record that in a config (e.g. in memory or a small JSON file) so future identical actions skip the prompt. Provide an option for one-time approval vs. “always allow this in the future.”
- **Path-Based Cascading Permissions:** Use a **directory-level permission model** for file operations. When a user approves write access to a folder, automatically allow write operations in its subdirectories without extra prompts. For example, granting write access to `/home/user/project` should implicitly allow modifications in `/home/user/project/src/...` files. This reduces prompt fatigue while maintaining security boundaries.
- **Tool-Specific and Command Permissions:** Introduce categories of permissions (modeled after Claude Code): e.g. _Tool usage permissions_ (using certain tools like a database tool might require trust), _Shell command permissions_ (running arbitrary commands may be gated separately), and _Filesystem permissions_ (read/write per path). Implement checks in a centralized `canUseTool(tool, params)` function that covers all these layers before tool execution.

**Testing & Validation:**

- Write unit tests for `needsPermissions` logic of each tool. For example, ensure an edit tool correctly returns true if editing a new file path without prior approval, and false if permission was previously granted.
- Simulate various usage flows in a CLI integration test: attempt a file edit on a new file and verify the system prompts for confirmation and blocks the edit until approved. Upon approval, confirm the edit proceeds. Then attempt a second edit on the same file (or in the same directory) to ensure no second prompt occurs due to the saved permission.
- Test negative cases: deny a permission and ensure the AI gracefully aborts that tool action (and informs the user or returns an appropriate message without crashing). Also test that read-only actions do **not** prompt the user and execute normally (unless a global setting to be extra safe).
- Security audit: verify that no tool can bypass the permission layer. For example, a shell tool should be wrapped such that even if the AI tries to run `rm -rf /` it will ask permission. Try malicious or destructive commands in a controlled environment to ensure the permission system catches them all.
- Check that the permission persistence respects scope: approving a high-level directory grants child access, while accessing a different path still triggers a prompt. Ensure denying an action doesn’t accidentally record a permission.

_Rationale:_ Incorporating a robust permission system at this stage is critical before TunaCode gains the ability to automatically modify code or execute commands. This phase brings TunaCode up to par with Claude Code’s **safety-first design**, where _“all tool use passes through a permissions system”_. By implementing persistent and path-scoped approvals, we improve usability (fewer repeated prompts) in line with Claude’s approach of cascading directory permissions. This builds user confidence that TunaCode will not make destructive changes without an explicit go-ahead, a foundation for trust as we expand the assistant’s capabilities.

## Phase 3: Persistent Project Context (Memory File)

Phase 3 focuses on **context management** – giving TunaCode a form of memory that persists across sessions for each project. We will introduce a special project context file (analogous to Claude Code’s `KODING.md`) to store useful information like frequent commands, user preferences, and project structure notes. This allows the AI to “remember” key details and avoids relearning the same context repeatedly.

**Key Tasks:**

- **Implement Context File Detection:** Decide on a file name and format (e.g. `TUNACODE.md` or reuse `KODING.md` convention). On startup or when switching into a project directory, have TunaCode check for this file. If present, automatically load its contents into the AI’s context/prompt so the assistant has immediate access to that information.
- **Define Content Structure:** Encourage a structured format in the context file for clarity. For example, sections for “Common Commands”, “Coding Conventions”, “Project Structure”, etc. This will help both the user and the AI parse the file. The content might be free-form Markdown, but structure (like a list of known commands) can be suggested.
- **Context Injection:** Merge the context file’s contents into the system or user prompt at conversation start. This could mean appending it to the system prompt (with a clear delimiter) or including it as a special “memory” message that the AI will see every time. Ensure this does not overflow token limits; if the file is large, consider summarizing or truncating less important parts.
- **AI-Led Updates:** Allow the AI to suggest updates to the context file. For example, if during a session the AI discovers a new important command or a coding style rule, it should (politely) ask the user whether to add it to the context file. Implement a tool or method for the AI to append to the file upon user approval (could be a specialized `UpdateContextFileTool`). This ties into the permission system – the user must confirm additions.
- **Session Continuity:** Ensure that the context file is reloaded in future sessions. If a user closes TunaCode and reopens it later in the same project, the information in `TUNACODE.md` should again be provided to the AI. This gives a continuous memory across distinct sessions.
- **Backwards Compatibility & Opt-Out:** Not all users may want a persistent context. Provide a way to disable or ignore the context file (via a command-line flag or config) in case the user prefers not to auto-load it. Also handle the case where the file doesn’t exist gracefully (just proceed without additional context).

**Testing & Validation:**

- **Functional Tests:** Start a TunaCode session in a project directory containing a pre-filled context file. Verify the AI can utilize information from this file by asking a question that relies on that info (e.g. if the file lists a frequent command `npm run build`, ask “how do I build the project?” and confirm the assistant uses the stored command without searching).
- **Update Flow:** Simulate a scenario where the AI learns something new: e.g., user asks the AI to run the test suite, AI finds the command `npm test` by searching, then suggests adding it to the context file. Test that the prompt is shown, user approves, and the file is updated with the new command. Then restart the session and ask for the test command again to ensure it’s remembered.
- **No-File Scenario:** Ensure starting TunaCode in a directory without a context file causes no errors. Possibly test that the AI might suggest creating one after a lot of interactions, but only if appropriate.
- **Opt-Out:** If a user opts out, test that the system indeed does not load or write any context file, and that the AI doesn’t reference non-existent memory.
- **Content Size Handling:** Create a dummy large context file and ensure the loading logic trims or summarizes it to avoid hitting token limits. The assistant should still function and not get confused by extremely large context input.

_Rationale:_ Providing persistent context greatly enhances the user experience by eliminating repetitive setup. This phase aligns with Claude Code’s use of `KODING.md` as a long-term memory store for each project. By remembering frequently used commands and preferences, TunaCode becomes more personalized and efficient over time. Implementing this after the core tool and permission framework ensures that we have the safety measures in place (since the AI could use this memory to execute commands). With context persistence, TunaCode will feel more “aware” of the project’s history, much like Claude Code does, leading to a smoother conversational workflow.

## Phase 4: Streaming Interactive UI with React/Ink

Phase 4 reimagines TunaCode’s interface as a **streaming, interactive CLI UI**. We will build a React-based terminal UI (using the [Ink](https://github.com/vadimdemedes/ink) library) to render the conversation, tool outputs, and prompts in real-time. This brings TunaCode’s UX in line with Claude Code’s rich terminal interface, which streams partial AI responses as they arrive. The new UI will also support input controls (for slash commands and permission dialogs) within the terminal.

**Key Tasks:**

- **Integrate React/Ink:** Scaffold a React component hierarchy for the CLI. Key components include: a text input box (for user prompts and command entry), a scrollable message log (displaying past user queries, AI responses, and tool usage results), and modals or inline components for special interactions (like permission approvals). Use Ink to manage rendering these in the terminal.
- **Streaming Response Rendering:** Modify the AI interaction loop to handle partial output. As the LLM generates a response stream (via an async iterator or similar), update the UI incrementally instead of waiting for completion. Claude Code’s UI renders partial responses token-by-token or chunk-by-chunk, and TunaCode should do the same. This likely involves using asynchronous generators: as each chunk arrives from the model, append it to the current message component so the user sees the answer forming in real time.
- **Asynchronous Tool Output Display:** Ensure that when tools execute (especially if read tools run in parallel), their outputs can stream into the UI as well. For example, if two file reads happen concurrently, the UI might show two loading spinners and then fill in each result as soon as it’s available. The UI needs to support updating specific message sections out-of-order (which Ink can handle via component state).
- **User Input Handling & History:** The input component should capture keystrokes, allow editing, and support history navigation (e.g. arrow keys to cycle through past commands). Claude Code’s `PromptInput.tsx` does this. Implement a similar mechanism so users can easily reuse previous queries or shell commands.
- **Permission Dialog UI:** Leverage Ink’s interactive components (like `<Box>`, `<Text>`, `<Button>`) to create a permission prompt UI element. When a permission is needed (from Phase 2’s system), instead of a simple console `y/n`, pop up a stylized box in the terminal showing the request (e.g. _“TunaCode wants to edit file X”_) with **Allow** / **Deny** buttons. The user can select an option (via arrow keys or a hotkey) and the UI will send the response back to the core logic to resume or cancel the action. Also display an option for “Always allow this” if permanent approval is possible.
- **Status Indicators:** Add subtle indicators for background activity – for instance, if tools are running in parallel or if the AI is thinking. This could be a spinner or progress bar at the bottom. When operations complete or are aborted, update the indicator.
- **Error and Cancellation Handling:** Ensure the UI remains stable on errors. If the user aborts an operation (maybe via Ctrl+C or a dedicated cancel command), use an `AbortController` to halt the async generators and update the UI (e.g. show “Operation cancelled”). Similarly, handle exceptions by displaying error messages in the message log without crashing the UI.

**Testing & Validation:**

- **Cross-Platform UI Test:** Run the new UI on different terminals (Linux, macOS, Windows if supported) to verify rendering is correct (layout, colors, input behavior). Check for any compatibility issues with control characters or resizing the terminal window.
- **Streaming Behavior:** Create a test mode or use a mock LLM that deliberately streams output slowly. Observe that the partial response appears progressively in the UI. Write an automated test that an artificial multi-chunk response results in multiple incremental DOM updates in the Ink components.
- **Permission UI:** Simulate a tool requiring permission and ensure the permission box appears with the correct message and that keyboard input (or button selection) properly registers an approval or denial. Test the “always allow” path saves the setting and that subsequent identical actions no longer trigger the dialog.
- **User Input Edge Cases:** Test multiline inputs (if allowed), very long input strings, rapid consecutive inputs, and history navigation (pressing up/down at various times). Ensure no keystrokes bleed into the output area and that focus management between components (input vs. buttons) is handled.
- **Performance:** Evaluate that the UI can handle a rapid stream of output (for example, if a tool prints a large file contents). The interface should remain responsive (maybe using backpressure if needed). Also, ensure memory usage doesn’t balloon with a long-running session – possibly implement a limit on stored message history or a way to compact it (ties in with `/compact` command in a later phase).
- **Fallback Mode:** If the terminal does not support advanced rendering or if React/Ink fails to initialize, TunaCode should detect this and fall back to a basic text-mode output, so the tool remains usable in minimal environments (this can be manual testing by simulating a dumb terminal).

_Rationale:_ A streaming UI significantly improves perceived responsiveness and aligns with modern CLI assistant design. Claude Code’s use of React/Ink allows rich interactions and live feedback – implementing this in TunaCode will similarly let users see progress in real time and interact with the assistant (e.g. granting permissions) in a more intuitive way. We tackle the UI in Phase 4 because it will also support upcoming features like slash commands (for which we need an input parser) and nicely rendered tool outputs. By completing the UI foundation here, subsequent phases can focus on functionality, using the new interface for display and control. This phase is also an architectural shift: moving from a simple REPL to an event-driven UI loop (e.g. a React `render()` loop with state). Thorough testing is emphasized to ensure the new UI remains robust across various conditions, delivering a polished user experience akin to Claude Code’s CLI.

## Phase 5: Parallel Execution Engine Enhancement

With the groundwork laid for classifying tools and a streaming UI in place, Phase 5 expands TunaCode’s **execution engine** to fully exploit parallelism for read-only operations. We will introduce an asynchronous generator-based execution manager (inspired by Claude Code’s `generators.all()` approach) to run multiple tool calls concurrently and aggregate their results in order. This phase aims to maximize performance for tasks like searching or reading many files, while ensuring thread-safe handling of any writes.

**Key Tasks:**

- **Async Generators for Tools:** Refactor the internal tool runner to use **async generators** for tool execution results. Each tool call can be treated as an async generator that yields intermediate results (especially if tools themselves stream output). Design a utility (e.g. `runToolsConcurrently(toolCalls[])`) that can launch multiple tool generators and yield from them as results come in. Claude Code’s architecture is built around async generators to enable parallel execution and streaming, and we will adopt a similar pattern in TunaCode.
- **`generators.all()` Utility:** Implement a helper similar to `generators.all()` which takes a list of async generator instances and orchestrates them. This utility should handle: starting all generators, listening for each to yield a value, and forwarding those results in a controlled way. It should also manage completion of all tasks and handle any thrown errors or cancellations. Essentially, this is a custom Promise.all for streaming results, ensuring one stalled generator doesn’t block others.
- **Parallel Read Execution:** Update the logic from Phase 1 that decided concurrency vs sequential. Now, if the check finds all tools are read-only, use the new `generators.all()` mechanism to **truly run them in parallel** and stream their outputs as they arrive. This might involve spawning each file read or search as a separate promise/async task. Maintain a cap on concurrency if needed to avoid overloading the system (e.g. only 5 file reads at once, queue the rest).
- **Sequential Write Execution:** Ensure that if any tool is not read-only, we do **not** use the parallel runner and instead use a `runToolsSerially()` path. Implement this function to execute each tool one after the other, awaiting completion of each (especially for write operations that must happen in order). This guarantees no two write operations overlap, preserving file integrity.
- **Result Collation and Ordering:** After parallel execution, collect all partial results and sort or merge them according to the original intended order of tool calls. For example, if the AI requested: (1) search for files, (2) read a file, (3) read another file – these might complete out-of-order in time, but we should present the combined results in the logical order 1-2-3 for consistency. Use identifiers or an index for each tool invocation to reorder outputs after all are done.
- **Concurrency Safeguards:** Add safeguards for edge cases: if a tool is flagged as read-only but in practice could have side effects, document or reconsider (in testing, adjust the flag accordingly). Incorporate cancellation logic so that if the user aborts (or if one tool fails critically), all ongoing tool tasks can be aborted promptly to free resources. Also ensure that the streaming UI properly handles interleaved outputs coming from multiple tasks (e.g. by tagging which output belongs to which tool in the UI).
- **Performance Tuning:** After implementing, profile the execution of typical multi-tool queries. Optimize the scheduling (e.g., adjust concurrency limits, batch certain operations) to ensure TunaCode is efficiently utilizing system resources without overwhelming the CPU or file system. Possibly integrate Node’s worker threads or a thread pool if needed for CPU-heavy tasks.

**Testing & Validation:**

- **Unit Tests for `generators.all()`:** Create dummy async generators (that yield known values after delays) and verify that our utility yields all values from all generators correctly and in deterministic order. Test scenarios with different ordering of completion, errors thrown mid-way, and cancellation signals to ensure robust behavior.
- **Integration Test - Parallel Reads:** Use a test project with multiple files and ask a question that prompts reading many files (e.g. “Find all occurrences of X across the repo”). Instrument the code to log timestamps when each file read starts and ends. Verify from logs or timing that multiple reads happened concurrently (overlapping times) and that the final assembled answer includes info from all files. The answer ordering should follow the logical sequence from the prompt, not the random order of completion.
- **Integration Test - Mixed Operations:** For a query that involves both read and write (e.g. “Search for A, then replace it with B in these files”), ensure the execution was sequential overall (no parallel writes). Check that the read portion might still use parallelism internally, but no write occurred until all reads done and user approved the change. After execution, validate the file changes are correct and no extra prompts triggered out of order.
- **Stress Test:** Try a large-scale operation, like reading 50 files concurrently. Monitor memory and CPU usage to ensure the system handles it gracefully (the concurrency limit should prevent exhausting resources). The system should remain responsive (especially if streaming UI is showing progress).
- **Error Handling:** Induce a failure in one of multiple parallel tools (e.g. make one file unreadable). Ensure that the error is reported (in the UI or logs) and that other parallel tasks still complete and yield their results. The final output should reflect the error for that part but still provide what it got from others. Likewise, test cancellation: start a long-running parallel task and issue a cancel, verify all tasks stop.

_Rationale:_ This phase takes full advantage of the groundwork from Phase 1 by introducing a sophisticated parallel execution engine. Claude Code’s responsiveness in large codebases comes from _“running all file search operations concurrently”_ and similar optimizations. By implementing `generators.all()` and related concurrency control, TunaCode will handle broad queries (like “search across the entire project”) much faster, closely mirroring Claude Code’s performance pattern. We implement this after establishing the UI and permissions so that the complexity of parallel feedback and user control is supported. With this completed, TunaCode’s tool processing will be both **fast** and **safe**, matching Claude Code’s hallmark capability of concurrent operations without race conditions.

## Phase 6: Slash Command Interface & Input Routing

Phase 6 adds a flexible **command interface** to TunaCode, allowing users to invoke special commands or run raw shell commands seamlessly alongside AI queries. We will introduce **slash commands** (prefixed with `/`) for internal TunaCode functions (like getting help or altering the session state) and support a prefix (e.g. `!`) for direct shell commands. This design follows Claude Code’s input processing system, where different input types are routed along different code paths for efficiency.

**Key Tasks:**

- **Input Parser & Router:** Implement a function (e.g. `processUserInput(input: string)`) to triage each input line. The logic should detect:

  - **Slash Commands:** If the input begins with `/`, treat it as a TunaCode command. Do not send it to the AI; instead, handle it internally.
  - **Shell Commands:** If the input begins with `!` (or another chosen prefix like `$`), execute the remainder of the line in a shell (using an existing Bash tool or spawning a subprocess). Also, do not involve the AI for these.
  - **Regular Queries:** If it’s anything else (normal text), process it as a prompt to the AI (potentially invoking tools via the AI’s plan).
    This separation ensures that purely operational commands don’t consume AI tokens and happen instantly.

- **Implement Core Slash Commands:** Provide a set of useful slash commands similar to Claude Code’s:

  - `/help` – display usage help, available commands, and tips for TunaCode.
  - `/compact` – trigger context compaction or summarization when the conversation is long (freeing up token space).
  - `/config` or `/settings` – open or print current configuration (like which model is used, permission settings, etc.).
  - `/history` – perhaps toggle or show recent conversation history (for user reference).
  - (Any other commands relevant to TunaCode’s domain, e.g. `/restart` to reset the session, etc.)
    These commands should be recognized and executed directly by TunaCode’s code. For example, `/help` might simply print a help message to the UI, and `/compact` could invoke an internal routine to summarize the conversation in memory.

- **Integrate with UI:** Since Phase 4 provided a React UI, ensure that when a slash command or shell command is entered, the UI knows to not send it to the AI loop. Instead, handle it in the main process. For shell commands (`!`), we can reuse the permission-guarded Bash tool under the hood, but from the user’s perspective it’s direct. Display the output of shell commands in the chat log (perhaps labeled or colorized differently to distinguish raw shell output).
- **Prevent AI Interference:** Adjust the system prompt or input processing so the AI is aware of these commands only conceptually. The AI should not see slash or bang commands as part of the conversation history (except maybe as a short system note that something was done). This avoids the AI trying to respond to them. Instead, handle them entirely outside the model. Claude Code notes that handling these outside the model improves performance and avoids wasted tokens.
- **Extend System Prompt (if needed):** You might include in the AI’s instructions that if the user input starts with certain prefixes, those are handled by the system – although ideally the AI never sees those at all. But ensure the AI doesn’t get confused if, say, the user asks about “/help” as a concept – in which case the AI can explain it as needed.
- **Internal Command Routing:** Design the slash command handling to be easily extensible. Perhaps maintain a registry or mapping of command strings to handler functions. This way new commands can be added without modifying a giant if/else. Also, allow passing arguments (e.g. `/config verbose` could be interpreted by splitting on whitespace).

**Testing & Validation:**

- **Slash Command Functionality:** Manually and with automated tests, try each implemented slash command. For `/help`, ensure it outputs a helpful message and does not involve the AI (no API call should be made). For `/compact`, populate a long conversation then invoke it and verify the context is reduced (and perhaps the UI prints a confirmation). If implementing `/history`, check it correctly fetches and displays recent turns.
- **Shell Command Execution:** Test inputs like `!pwd` or `!echo hello`. They should execute immediately via the shell tool and display the command’s output under the correct user identity in the log. Confirm that if a shell command tries to do something disallowed (e.g. `!rm file`), the permission system from Phase 2 still intercepts it (the Bash tool should be subject to permissions).
- **Routing Logic:** Write unit tests for the input parser to assert that various inputs route correctly: `"!ls"` → shell execution, `"/help"` → internal command, `"What is 2+2?"` → AI query, etc. Include edge cases like just `"!"` or `"/"` with no following text (should handle gracefully, maybe treat as help for commands).
- **No Interference with AI:** Confirm that using slash or shell commands does not break the conversation state. For example, do a normal AI query after a few shell commands and ensure the AI’s answer is unaffected by those prior shell outputs. The AI should continue the dialogue normally. Conversely, test that if the user asks the AI something that looks like a command (e.g. “What does /compact do?”), the AI can answer (likely from its training or a predefined explanation), rather than the system intercepting it – meaning the parser should only catch inputs starting at the beginning with `/` or `!`.
- **User Guidance:** Verify that the `/help` output indeed lists these commands and how to use them, so users know about the feature. Possibly also test that typing an unknown slash command (e.g. `/foobar`) returns an error message like “Unrecognized command” rather than being sent to AI unhandled.
- **Concurrency Check:** Ensure that commands can be issued even while an AI response is streaming (if you choose to support that via multi-threading or by queueing inputs). It might be acceptable that user must wait until the AI finishes before entering a new command, but if not, test accordingly for race conditions.

_Rationale:_ Adding slash commands and direct shell escapes will make TunaCode far more versatile and user-friendly. This design mirrors Claude Code’s approach of **multiple input modes** – _“Bash commands and slash commands don’t waste tokens or require AI processing, while AI-directed queries get full context and tools”_. By implementing this in TunaCode, we achieve both **flexibility** (letting advanced users directly run known commands or control the session) and **performance** (no unnecessary LLM calls for simple operations). This phase is timed after the UI is ready because a robust input system and UI feedback are needed to properly support these commands (e.g., showing help, printing shell output nicely). Upon completion, TunaCode will support the same rich command palette as Claude’s CLI, making the interface more powerful and intuitive.

## Phase 7: Recursive Multi-step Query Handling

In Phase 7, we will enable **recursive tool usage**, allowing the AI to carry out multi-step tasks autonomously within a single user query. This means the AI can plan a series of actions (e.g. search for a bug, read the file, then edit the code, then run tests) and execute them one by one, using results from each step to inform the next – all without further user prompting. This behavior is a hallmark of Claude Code’s agent, where the entire tool system is recursive, enabling seamless complex operations.

**Key Tasks:**

- **Conversation Loop Enhancements:** Modify the main AI loop to support iterative reasoning cycles. After the AI’s initial response to a user query, check if it requested any tool actions (this likely is already happening from earlier phases). The new part is to allow the AI to continue generating **after** the tools have run, treating the tool outputs as new context. Implement logic such that once tool results are obtained, they are fed back into the model (appended to the conversation) and the model is prompted to continue its answer or decide the next action. This may involve calling the AI API multiple times within one high-level query cycle – effectively a loop of _“think → act (tool) → observe (result) → think further → … → final answer.”_
- **Recursive Generator Pattern:** Implement the AI reasoning as an async generator that can yield either a chunk of final answer text or a tool invocation, and can be resumed with new information. For instance, the AI might yield a “ToolUse” object for a file read; our system executes it, then we send the result back into the AI’s context and resume the generator to get the next part of the answer. Continue until the AI signals it is done answering. This pattern is described in Claude Code’s design, allowing the AI to _“continue the conversation with this new information”_ in a loop.
- **State Management:** Keep track of intermediate state. As multiple tools are used in sequence, maintain a transcript of all tool actions and results as part of the conversation history (so the AI remembers what it did). Possibly mark these in the UI distinctly (e.g. each tool use and its result appears inline in the chat). Ensure that after the final answer is given, the conversation state includes all the steps for future reference or follow-up questions.
- **Tool Response Injections:** Decide how to format the injection of tool outputs when sending back to the LLM. For example, if the AI said: “`[reads file X]`”, after we get the content of X, we might insert a system or assistant message like: “Content of X is: ... (file text) ...”. The prompt format should likely mimic how Claude Code does it so the AI can parse it. (This might already be partially implemented in TunaCode if it had some tool use mechanism, but we ensure it handles multiple rounds.)
- **Loop Termination Conditions:** Make sure there’s a clear condition for when to stop the recursive loop. Usually, if the AI’s response doesn’t request any new tool action, that means it’s presenting final information to the user, and we can consider the query complete. Implement checks to break out if, say, too many steps occur (to prevent infinite loops or runaway processes). For example, after a certain number of tool uses in one query (maybe 5 or 10), require user confirmation to continue, to avoid an accidental loop.
- **Leverage Permissions & UI:** This recursive execution must still respect the permission system – e.g., if in step 3 the AI decides to edit a file, it should trigger the Phase 2 permission prompt as usual. The UI from Phase 4 will be displaying each step’s outputs as they stream in, so the user stays informed. They will essentially watch the AI go through the steps (with outputs appearing as it goes), until it arrives at an answer. This is analogous to seeing Claude Code _“ask a question, read files to find the answer, use the info to solve a problem, suggest and implement changes, and verify the changes worked…all in a single seamless interaction”_. TunaCode will achieve the same seamless multi-step workflow in this phase.

**Testing & Validation:**

- **End-to-End Complex Task:** Write an integration test simulating a scenario such as: “**Find and fix bugs in file `Bug.tsx`**” (similar to examples in the Claude Code guide). The expected behavior: the AI will search for the file, read it, identify an issue, propose an edit, apply the edit, then perhaps run tests. Verify through logs or the UI that each step was executed in sequence with no additional user input. Check that after these steps, the final assistant answer is correct (e.g. “I fixed the bug by doing X”) and the file was indeed modified appropriately.
- **Observe Conversation Consistency:** Ensure that all intermediate steps (tool uses and results) are being properly recorded in the conversation context given to the AI. One way to test is to ask a follow-up question that references something from a tool result and see if the AI remembers it. For instance, after the above multi-step fix, ask “How did you fix it?” – the AI should recall the changes it made (since the conversation history included that).
- **Loop Control:** Create a scenario where the AI might tend to loop or do many steps (perhaps an ill-defined task). Test that our loop termination or user confirmation triggers correctly. For example, if the AI calls tools in a loop (bug in prompt handling), ensure it doesn’t spin forever – maybe inject a fail-safe after N steps. This might be tested by a dummy tool that always asks for another tool usage to see if we break out.
- **Permission Interruption:** Test a case where one of the steps requires permission and the user denies it. The system should handle this gracefully – possibly aborting the entire multi-step process or having the AI catch the rejection and respond (e.g. “I cannot proceed without permission to X”). The conversation shouldn’t hang; ensure the AI is informed of the denial (maybe via an error result) and can conclude accordingly.
- **Parallel vs Sequential Mix:** If the AI triggers multiple tools concurrently in one step (less likely in recursive mode, but possible if it plans multiple reads together), verify that Phase 5’s parallel execution still works within the loop. For example, the AI might request two files to be read at once as part of its plan; ensure the engine runs them concurrently and then continues to the next step.
- **Resource Cleanup:** Ensure that after a recursive multi-step query completes, all resources (promises, file handles, subprocesses) from that sequence are cleaned up. Run a stress test of several multi-step queries in a row to watch for memory leaks or orphaned processes.

_Rationale:_ This phase unlocks the “agentic” power of TunaCode, enabling it to tackle complex requests autonomously. By **recursively using tool results to inform further actions**, TunaCode will closely emulate Claude Code’s ability to handle multi-step tasks in one go. Implementing this after establishing the tool execution, UI, and permission frameworks is crucial – those systems now work together to support such autonomy safely. With recursion, the assistant can for example diagnose a problem, fetch data from code, apply a fix, and verify the fix, all within one user command, which is exactly the kind of advanced capability Claude’s CLI demonstrates. This dramatically improves TunaCode’s problem-solving efficiency and brings it to feature parity with the Claude Code architecture.

## Phase 8: System Prompt & Agent Behavior Alignment

The final phase fine-tunes TunaCode’s **AI persona and prompting** to match the tone, formatting, and behavior guidelines observed in Claude Code’s CLI agent. By updating the system prompt and related settings, we ensure the assistant communicates concisely, uses proper markdown formatting, and adheres to the desired style and safety rules.

**Key Tasks:**

- **Adopt Claude Code’s Tone Guidelines:** Rewrite TunaCode’s system prompt (the initial instructions given to the AI each session) to include clear guidance on style. Claude Code’s agent is instructed to be _“concise, direct, and to the point”_, especially given the CLI environment. TunaCode should similarly avoid verbose explanations or unnecessary preambles. Include directives to _not_ produce long apologies or extra commentary – just answer or act as needed. Emphasize brevity (answers ideally a few lines at most, unless more detail is explicitly asked for).
- **Explain Actions When Appropriate:** In the prompt, mention that when the assistant runs a non-trivial command or makes a significant change, it should briefly explain _what it’s doing and why_. This keeps the user informed (important for CLI tools that might alter the system). E.g., if the AI uses a `rm` command or performs a database migration, it should output a one-liner rationale. Claude’s prompt encourages explaining potentially impactful commands to ensure the user isn’t surprised.
- **Markdown and Formatting Rules:** Instruct the AI to use GitHub-Flavored Markdown for any code snippets or structured data in answers (which our CLI UI will render appropriately). Also, remind it that the output is monospaced in a terminal, so formatting should be clean (no giant tables unless needed, etc.). Ensure the prompt clarifies that all normal response text will be shown to the user, whereas tool outputs should be the result of using tools (so the AI shouldn’t fake tool output). Claude Code’s system prompt contains similar guidance on formatting and not revealing system-level details.
- **No Unnecessary Preamble/Postamble:** Explicitly tell the AI to avoid phrases like _“The answer is:”_ or _“Here’s what I found:”_. It should jump straight to the answer or action result. Also instruct it not to repeat the question in the answer. Claude Code’s agent guidelines stress _no extra wrapping text_ around answers, given it’s a CLI where brevity matters.
- **Refusal and Safe Completion Behavior:** Incorporate any needed alignment with Claude Code’s approach to refusals or safe-completions. For instance, if the AI cannot do something due to permissions or policy, it should respond briefly without lecturing (Claude’s prompt: _“do not say why or what it could lead to…comes across as preachy”_, and _“offer alternatives if possible, otherwise a brief response”_). Make sure TunaCode’s AI similarly handles these cases gracefully and minimally.
- **Integrate New Features into Prompt:** Update the prompt to mention the existence of tools, commands, and context file, so the AI knows about them. For example, instruct the AI that if it needs to know project-specific info it can look at the context file content (which we now prepend), or that if it needs to run a command it should output the appropriate tool usage format (assuming our system uses some notation). Essentially, retrain the prompt with a condensed “capabilities list” so the AI fully leverages the features we added.
- **Tone Consistency Testing:** Possibly implement a feedback loop: have the AI generate a few example outputs with the new prompt, review if they meet the style guidelines, and tweak wording as needed. For example, test with queries of different types (straight Q\&A, code generation, asking for a dangerous action that should be refused, etc.) to see that the style and safety responses are as expected.

**Testing & Validation:**

- **Style Compliance Tests:** Prepare a set of sample user questions and desired styled answers. Run TunaCode with the new prompt in a dry-run mode (or using a stubbed model that echoes decisions) to see how it responds. Check that answers are concise and formatted correctly (e.g. code in markdown \`\`\` blocks, lists when needed, etc.). Adjust the prompt instructions if certain undesired patterns appear.
- **Realistic Conversations:** Simulate a full realistic session: open TunaCode in a project, ask it to do a series of tasks (maybe covering searching, editing, asking conceptual questions). Observe the tone – ensure it’s consistently professional and terse. The assistant should, for instance, say “Done.” after a quick file edit, rather than a long explanation. If any verbosity or off-style responses happen, refine the instructions and test again.
- **Edge Case Queries:** Test cases such as: asking math or factual questions (should answer directly with one line), asking it to run a dangerous command (should either refuse with a brief line or ask for permission due to earlier phases), and asking for help (should show concise help message with markdown formatting if appropriate). Each of these should reflect the Claude-like tone (e.g. the help output should not be overly wordy either).
- **No Regression in Functionality:** Verify that the new system prompt still allows all tools and features to function. The AI must still recall to use tools when needed. For example, ask “what files are in directory X?” – the AI should respond by using the ls tool (and the output being shown), not by trying to guess. If the new prompt caused the AI to become too terse or not use tools, we may need to explicitly mention the tool usage format in the instructions.
- **User Feedback Phase:** If possible, have a few users or team members use the updated TunaCode and gather subjective feedback on the AI’s demeanor and helpfulness. Ensure the changes indeed make it feel more like Claude Code (use this to make any final tuning).

_Rationale:_ Fine-tuning the AI’s prompt and behavior is the capstone to make TunaCode truly feel like Claude Code. Claude’s CLI agent is known for its disciplined style – _concise answers, no fluff, explaining actions when needed, and using a friendly yet efficient tone_. By encoding similar rules, we align TunaCode’s user experience with that standard. This phase is done last because it relies on all prior functionality being in place (the prompt will reference tools, commands, context – all implemented by now). Adjusting the prompt at the end ensures we don’t have to continually revise it as new capabilities are added. Once Phase 8 is complete, TunaCode should not only function like Claude Code under-the-hood, but also **feel** like Claude Code in how it interacts with the user – achieving the goal of replicating as much of Claude Code’s functionality and UX as possible.

---

By following these phases in order, TunaCode will undergo a transformation: from a basic coding assistant to an **agentic, Claude Code-like CLI assistant**. Each phase builds upon the previous, adding layers of capability – from concurrency and safety, to memory, to UI/UX enhancements, to advanced autonomy and refined behavior. Throughout the process, careful testing and iterative refinement ensure stability. Upon completion, users of TunaCode can expect near feature-parity with Claude’s CLI, including real-time streamed answers, safe direct code manipulation, contextual awareness across sessions, and a snappy command-driven interface, all delivered with a professional concise tone. This phased roadmap balances the development effort by tackling foundational architectural changes first, then gradually layering on user-facing improvements, culminating in a robust and user-aligned AI coding assistant system.

**Sources:**

- Gerred Dillon, _The Agentic Systems Series – Building an Agentic System (Book 1)_, “Claude Code Architecture” (Anthropic CLI case study) – key patterns and design decisions.
- Gerred Dillon, _The Agentic Systems Series – Building an Agentic System (Book 1)_, “Tool Permissions and Security” – Claude Code’s permission model with user approvals and cascading directory trust.
- Gerred Dillon, _The Agentic Systems Series – Building an Agentic System (Book 1)_, “Context and Memory (KODING.md)” – persistent project context file for Claude Code.
- Gerred Dillon, _The Agentic Systems Series – Building an Agentic System (Book 1)_, “Command System Design” – slash commands and input routing in Claude Code.
- Gerred Dillon, _The Agentic Systems Series – Building an Agentic System (Book 1)_, “Agent Behavior and Tone” – excerpt of Claude Code’s CLI assistant system prompt (style guidelines).
